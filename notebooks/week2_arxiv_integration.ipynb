{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b84551ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WEEK 2 PREREQUISITE CHECK\n",
      "==================================================\n",
      "✓ FastAPI: Healthy\n",
      "✓ PostgreSQL (via API): Healthy\n",
      "✓ Ollama: Healthy\n",
      "✓ OpenSearch: Healthy\n",
      "✓ Airflow: Healthy\n",
      "\n",
      "All services healthy! Ready for Week 2 development.\n"
     ]
    }
   ],
   "source": [
    "# Test Service Connectivity\n",
    "import requests\n",
    "import subprocess\n",
    "import json\n",
    "\n",
    "services_to_test = {\n",
    "    \"FastAPI\": \"http://localhost:8000/api/v1/health\",\n",
    "    \"PostgreSQL (via API)\": \"http://localhost:8000/api/v1/health\",\n",
    "    \"Ollama\": \"http://localhost:11434/api/version\",\n",
    "    \"OpenSearch\": \"http://localhost:9200/_cluster/health\",\n",
    "    \"Airflow\": \"http://localhost:8080/health\",\n",
    "}\n",
    "\n",
    "print(\"WEEK 2 PREREQUISITE CHECK\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "all_healthy = True\n",
    "\n",
    "for service_name, url in services_to_test.items():\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            print(f\"✓ {service_name}: Healthy\")\n",
    "        else:\n",
    "            print(f\"✗ {service_name}: HTTP {response.status_code}\")\n",
    "            all_healthy = False\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(f\"✗ {service_name}: Not accessible\")\n",
    "        all_healthy = False\n",
    "    except Exception as e:\n",
    "        print(f\"✗ {service_name}: {type(e).__name__}\")\n",
    "        all_healthy = False\n",
    "\n",
    "print()\n",
    "if all_healthy:\n",
    "    print(\"All services healthy! Ready for Week 2 development.\")\n",
    "else:\n",
    "    print(\"Some services need attention. Check Week 1 notebook.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e2c62bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys.path[0:3]: ['/Users/surekha/Documents/projects/RAG/research-assistant/src', '/Users/surekha/Documents/projects/RAG/research-assistant', '/Users/surekha/.local/share/uv/python/cpython-3.12.12-macos-aarch64-none/lib/python312.zip']\n"
     ]
    }
   ],
   "source": [
    "# ensure repo root and src are on sys.path\n",
    "import sys, pathlib\n",
    "\n",
    "repo_root = pathlib.Path().resolve()\n",
    "# if notebooks live in a folder, adjust repo_root = repo_root.parent etc.\n",
    "if (repo_root / \"src\").exists():\n",
    "    sys.path.insert(0, str(repo_root))\n",
    "    sys.path.insert(0, str(repo_root / \"src\"))\n",
    "else:\n",
    "    for p in repo_root.parents:\n",
    "        if (p / \"src\").exists():\n",
    "            sys.path.insert(0, str(p))\n",
    "            sys.path.insert(0, str(p / \"src\"))\n",
    "            break\n",
    "print(\"sys.path[0:3]:\", sys.path[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "beb743b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing ARXIV API Client\n",
      "========================================\n",
      " Client created: https://export.arxiv.org/api/query\n",
      " Rate limit: 3.0\n",
      " Max results: 15\n",
      " Category: cs.AI\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from src.services.arxiv.factory import make_arxiv_client\n",
    "\n",
    "print(\"Testing ARXIV API Client\")\n",
    "print(\"=\" * 40)\n",
    "arxiv_client = make_arxiv_client()\n",
    "print(f\" Client created: {arxiv_client.base_url}\")\n",
    "print(f\" Rate limit: {arxiv_client.rate_limit_delay}\")\n",
    "print(f\" Max results: {arxiv_client.max_results}\")\n",
    "print(f\" Category: {arxiv_client.search_category}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6fcb191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1: Fetch Recent cs.AI papers\n",
      "Fetched 1 papers\n",
      "   1. [2512.05117v1] The Universal Weight Subspace Hypothesis...\n",
      "      Authors: Prakhar Kaushik, Shravan Chaudhari...\n",
      "      Categories: cs.LG, cs.AI, cs.CV\n",
      "      Published: 2025-12-04T18:59:58Z\n",
      "\n"
     ]
    }
   ],
   "source": [
    "async def test_fetch_papers():\n",
    "    \"\"\"Testing fetching papers from arxiv client with rate limiting\"\"\"\n",
    "\n",
    "    print(\"Test 1: Fetch Recent cs.AI papers\")\n",
    "    try:\n",
    "        papers = await arxiv_client.fetch_papers(\n",
    "            max_results=1, sort_by=\"submittedDate\", sort_order=\"descending\"\n",
    "        )\n",
    "\n",
    "        print(f\"Fetched {len(papers)} papers\")\n",
    "\n",
    "        if papers:\n",
    "            for i,paper in enumerate(papers[:2],1):\n",
    "                print(f\"   {i}. [{paper.arxiv_id}] {paper.title[:60]}...\")\n",
    "                print(f\"      Authors: {', '.join(paper.authors[:2])}{'...' if len(paper.authors) > 2 else ''}\")\n",
    "                print(f\"      Categories: {', '.join(paper.categories)}\")\n",
    "                print(f\"      Published: {paper.published_date}\")\n",
    "                print()\n",
    "\n",
    "        return papers\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching papers: {e}\")\n",
    "        if \"503\" in str(e):\n",
    "            print(\" arxiv API temporarily unavailable(normal)\")\n",
    "            print(\" Rate limiting and error handling working correctly\")\n",
    "        return []\n",
    "\n",
    "papers = await test_fetch_papers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "890bb0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 2: Date Range Filtering\n",
      " Date filtering test: 2 papers from 20250808-20250809\n",
      "   1. [2508.07111v1] Investigating Intersectional Bias in Large Language Models u...\n",
      "      Authors: Falaah Arif Khan, Nivedha Sivakumar...\n",
      "      Categories: cs.CL, cs.AI\n",
      "      Published: 2025-08-09T22:24:40Z\n",
      "\n",
      "   2. [2508.07107v2] Designing a Feedback-Driven Decision Support System for Dyna...\n",
      "      Authors: Timothy Oluwapelumi Adeyemi, Nadiah Fahad AlOtaibi\n",
      "      Categories: cs.AI, cs.CY\n",
      "      Published: 2025-08-09T21:24:54Z\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test date filtering\n",
    "async def test_date_filtering():\n",
    "\n",
    "    print(\"Test 2: Date Range Filtering\")\n",
    "\n",
    "    from_date = \"20250808\"\n",
    "    to_date = \"20250809\"\n",
    "\n",
    "    try:\n",
    "        date_papers = await arxiv_client.fetch_papers(max_results=2, from_date=from_date,to_date=to_date)\n",
    "\n",
    "        print(f\" Date filtering test: {len(date_papers)} papers from {from_date}-{to_date}\")\n",
    "        if date_papers:\n",
    "            for i, paper in enumerate(date_papers, 1):\n",
    "                print(f\"   {i}. [{paper.arxiv_id}] {paper.title[:60]}...\")\n",
    "                print(\n",
    "                    f\"      Authors: {', '.join(paper.authors[:2])}{'...' if len(paper.authors) > 2 else ''}\"\n",
    "                )\n",
    "                print(f\"      Categories: {', '.join(paper.categories)}\")\n",
    "                print(f\"      Published: {paper.published_date}\")\n",
    "                print()\n",
    "\n",
    "        return date_papers\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Date filtering error: {e}\")\n",
    "        return []\n",
    "\n",
    "# Run date filtering test\n",
    "date_papers = await test_date_filtering()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "964dbb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 3: PDF Download & Caching\n",
      "Testing PDF download for : 2508.07111v1\n",
      "Title: Investigating Intersectional Bias in Large Language Models u\n",
      "data/arxiv_pdfs/2508.07111v1.pdf\n",
      "PDF downloaded 2508.07111v1.pdf (6.81) MB\n"
     ]
    }
   ],
   "source": [
    "# test pdf download and caching locally\n",
    "async def test_pdf_download(test_papers):\n",
    "\n",
    "    print(\"Test 3: PDF Download & Caching\")\n",
    "\n",
    "    if not test_papers:\n",
    "        print(\"No papers available for PDF download test\")\n",
    "\n",
    "    test_paper = test_papers[0]\n",
    "    print(f\"Testing PDF download for : {test_paper.arxiv_id}\")\n",
    "    print(f\"Title: {test_paper.title[:60]}\")\n",
    "\n",
    "    try:\n",
    "        pdf_path = await arxiv_client.download_pdf(test_paper)\n",
    "        print(pdf_path)\n",
    "        if pdf_path and pdf_path.exists():\n",
    "            size_mb = pdf_path.stat().st_size / (1024*1024)\n",
    "            print(f\"PDF downloaded {pdf_path.name} ({size_mb:.2f}) MB\")\n",
    "            return pdf_path\n",
    "        else:\n",
    "            print(\"PDF downlod failed\")\n",
    "            return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"PDF download failed\")\n",
    "        return None\n",
    "    \n",
    "pdf_path = await test_pdf_download(date_papers[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "304cff65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-06 12:03:48,293 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-12-06 12:03:48,351 - INFO - Going to convert document batch...\n",
      "2025-12-06 12:03:48,351 - INFO - Initializing pipeline for StandardPdfPipeline with options hash 70256a236a6856c82de2c96fe229a58e\n",
      "2025-12-06 12:03:48,358 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-12-06 12:03:48,360 - INFO - Registered picture descriptions: ['vlm', 'api']\n",
      "2025-12-06 12:03:48,365 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-12-06 12:03:48,367 - INFO - Registered ocr engines: ['auto', 'easyocr', 'ocrmac', 'rapidocr', 'tesserocr', 'tesseract']\n",
      "2025-12-06 12:03:48,400 - INFO - Accelerator device: 'mps'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing PDF parsing\n",
      "\n",
      " Found 1 PDF files to test parsing\n",
      "Testing PDF parsing with: 2508.07111v1.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-06 12:03:50,312 - INFO - Accelerator device: 'mps'\n",
      "2025-12-06 12:03:50,890 - INFO - Processing document 2508.07111v1.pdf\n",
      "2025-12-06 12:04:11,727 - INFO - Finished converting document 2508.07111v1.pdf in 23.44 sec.\n",
      "2025-12-06 12:04:11,892 - WARNING - Parameter `strict_text` has been deprecated and will be ignored.\n",
      "2025-12-06 12:04:11,894 - INFO - Parsed data/arxiv_pdfs/2508.07111v1.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " PDF parsing successful\n",
      "Sections: 23\n",
      "Raw text length: 85167 characters\n",
      "Parser used: ParserType.DOCLING\n",
      " First section: Content, (84) chars\n"
     ]
    }
   ],
   "source": [
    "# Testing PDF parsing using docling\n",
    "\n",
    "from src.services.pdf_parser.factory import make_pdf_parser_service\n",
    "from src.config import get_settings\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"Testing PDF parsing\")\n",
    "\n",
    "pdf_parser = make_pdf_parser_service()\n",
    "cache_dir = Path(\n",
    "    \"data/arxiv_pdfs\"\n",
    ")\n",
    "\n",
    "if cache_dir.exists():\n",
    "    pdf_files = list(cache_dir.glob(\"*.pdf\"))\n",
    "    print(f\"\\n Found {len(pdf_files)} PDF files to test parsing\")\n",
    "\n",
    "    if pdf_files:\n",
    "        test_pdf = pdf_files[0]\n",
    "        print(f\"Testing PDF parsing with: {test_pdf.name}\")\n",
    "\n",
    "        try:\n",
    "            pdf_content = await pdf_parser.parse_pdf(test_pdf)\n",
    "\n",
    "            if pdf_content:\n",
    "                print(f\" PDF parsing successful\")\n",
    "                print(f\"Sections: {len(pdf_content.sections)}\")\n",
    "                print(f\"Raw text length: {len(pdf_content.raw_text)} characters\")\n",
    "                print(f\"Parser used: {pdf_content.parser_used}\")\n",
    "\n",
    "            if pdf_content.sections:\n",
    "                first_section = pdf_content.sections[0]\n",
    "                print(f\" First section: {first_section.title}, ({len(first_section.content)}) chars\")\n",
    "            else:\n",
    "                print(\"✗ PDF parsing failed (Docling compatibility issue)\")\n",
    "                print(\"This is expected - not all PDFs work with Docling\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"✗ PDF parsing error: {e}\")\n",
    "            print(\"This demonstrates the error handling in action\")\n",
    "    else:\n",
    "        print(\"No PDF files available for parsing test\")\n",
    "else:\n",
    "    print(\"No PDF cache directory found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc25d6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-06 12:04:12,031 - INFO - Attempting to connect to PostgreSQL at: localhost:5432/rag_db\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 5: Database Storage\n",
      "postgresql+psycopg2://rag_user:rag_password@localhost:5432/rag_db\n",
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-06 12:04:12,178 - INFO - Database connection test successfully\n",
      "2025-12-06 12:04:12,188 - INFO - All tables already exist - no new tables created\n",
      "2025-12-06 12:04:12,188 - INFO - PostgreSQL database initialized successfully\n",
      "2025-12-06 12:04:12,188 - INFO - Database: rag_db\n",
      "2025-12-06 12:04:12,189 - INFO - Total tables: log, dag_priority_parsing_request, job, slot_pool, callback_request, dag_code, dag_pickle, ab_user, ab_register_user, connection, sla_miss, variable, import_error, serialized_dag, dataset_alias, dataset_alias_dataset, dataset, dataset_alias_dataset_event, dataset_event, dag_schedule_dataset_alias_reference, dag, dag_schedule_dataset_reference, task_outlet_dataset_reference, dataset_dag_run_queue, log_template, dag_run, dag_tag, dag_owner_attributes, ab_permission, ab_permission_view, ab_view_menu, ab_user_role, ab_role, dag_warning, dagrun_dataset_event, trigger, task_instance, dag_run_note, ab_permission_view_role, rendered_task_instance_fields, task_fail, task_map, task_reschedule, xcom, task_instance_note, task_instance_history, session, alembic_version, papers\n",
      "2025-12-06 12:04:12,190 - INFO - Database connection established\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database connection created\n",
      "Storing paper: 2512.05117v1\n",
      "<src.models.paper.Paper object at 0x378891250>\n",
      "Paper created with ID: 3b71bdc4-d4b8-47cf-808b-64af09ac6cda\n",
      "Database ID: 3b71bdc4-d4b8-47cf-808b-64af09ac6cda\n",
      "arXiv ID: 2512.05117v1\n",
      "Title: The Universal Weight Subspace Hypothesis...\n",
      "Authors: 5 authors\n",
      "Categories: cs.LG, cs.AI, cs.CV\n",
      "Paper retrieveal test passes\n"
     ]
    }
   ],
   "source": [
    "# database storage testing\n",
    "from src.db.factory import make_database\n",
    "from src.repositories.paper import PaperRepository\n",
    "from src.schemas.arxiv.paper import PaperCreate\n",
    "from dateutil import parser as date_parser\n",
    "\n",
    "print(\"Test 5: Database Storage\")\n",
    "\n",
    "settings = get_settings()\n",
    "database = make_database()\n",
    "print(\"Database connection created\")\n",
    "\n",
    "if papers:\n",
    "    test_paper = papers[0]\n",
    "    print(f\"Storing paper: {test_paper.arxiv_id}\")\n",
    "\n",
    "    try:\n",
    "        with database.get_session() as session:\n",
    "            paper_repo = PaperRepository(session)\n",
    "\n",
    "            published_date = date_parser.parse(test_paper.published_date) if isinstance(test_paper.published_date, str) else test_paper.published_date\n",
    "            \n",
    "            paper_create = PaperCreate(\n",
    "                arxiv_id=test_paper.arxiv_id,\n",
    "                title=test_paper.title,\n",
    "                authors=test_paper.authors,\n",
    "                abstract=test_paper.abstract,\n",
    "                categories=test_paper.categories,\n",
    "                published_date=published_date,\n",
    "                pdf_url=test_paper.pdf_url,\n",
    "            )\n",
    "            \n",
    "            \n",
    "            stored_paper = paper_repo.upsert(paper_create)\n",
    "            print(stored_paper)\n",
    "            if stored_paper:\n",
    "                print(f\"Paper created with ID: {stored_paper.id}\")\n",
    "                print(f\"Database ID: {stored_paper.id}\")\n",
    "                print(f\"arXiv ID: {stored_paper.arxiv_id}\")\n",
    "                print(f\"Title: {stored_paper.title[:50]}...\")\n",
    "                print(f\"Authors: {len(stored_paper.authors)} authors\")\n",
    "                print(f\"Categories: {', '.join(stored_paper.categories)}\")\n",
    "\n",
    "                retrieved_paper = paper_repo.get_by_arxiv_id(test_paper.arxiv_id)\n",
    "                if retrieved_paper:\n",
    "                    print(\"Paper retrieveal test passes\")\n",
    "                else:\n",
    "                    print(\"Paper retrieval failed\")\n",
    "            else:\n",
    "                print(\"Paper storage failed\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Database error: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"No papers available for database storage test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f912344",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-06 12:04:12,212 - INFO - Fetching 10 cs.AI papers from arXiv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 6: Complete Metadata Fetcher Pipeline\n",
      "==================================================\n",
      "✓ Metadata fetcher service created\n",
      "Running small batch test (2 papers, no PDF processing for speed)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-06 12:04:12,276 - INFO - HTTP Request: GET https://export.arxiv.org/api/query?search_query=cat:cs.AI&start=0&max_results=10&sortBy=submittedDate&sortOrder=descending \"HTTP/1.1 200 OK\"\n",
      "2025-12-06 12:04:12,283 - INFO - Fetched 10 papers\n",
      "2025-12-06 12:04:12,284 - INFO - Starting async pipeline for 10 PDFs\n",
      "2025-12-06 12:04:12,284 - INFO - Concurrent downloads: 5\n",
      "2025-12-06 12:04:12,284 - INFO - Concurrent parsing: 1\n",
      "2025-12-06 12:04:12,284 - INFO - Downloaded pdf path for paper: 2512.05117v1: data/arxiv_pdfs/2512.05117v1.pdf\n",
      "2025-12-06 12:04:12,285 - INFO - Downloading PDF from https://arxiv.org/pdf/2512.05117v1\n",
      "2025-12-06 12:04:12,285 - INFO - Downloaded pdf path for paper: 2512.05112v1: data/arxiv_pdfs/2512.05112v1.pdf\n",
      "2025-12-06 12:04:12,285 - INFO - Downloading PDF from https://arxiv.org/pdf/2512.05112v1\n",
      "2025-12-06 12:04:12,285 - INFO - Downloaded pdf path for paper: 2512.05110v1: data/arxiv_pdfs/2512.05110v1.pdf\n",
      "2025-12-06 12:04:12,286 - INFO - Downloading PDF from https://arxiv.org/pdf/2512.05110v1\n",
      "2025-12-06 12:04:12,286 - INFO - Downloaded pdf path for paper: 2512.05105v1: data/arxiv_pdfs/2512.05105v1.pdf\n",
      "2025-12-06 12:04:12,286 - INFO - Downloading PDF from https://arxiv.org/pdf/2512.05105v1\n",
      "2025-12-06 12:04:12,287 - INFO - Downloaded pdf path for paper: 2512.05103v1: data/arxiv_pdfs/2512.05103v1.pdf\n",
      "2025-12-06 12:04:12,287 - INFO - Downloading PDF from https://arxiv.org/pdf/2512.05103v1\n",
      "2025-12-06 12:04:15,387 - INFO - HTTP Request: GET https://arxiv.org/pdf/2512.05105v1 \"HTTP/1.1 200 OK\"\n",
      "2025-12-06 12:04:15,388 - INFO - HTTP Request: GET https://arxiv.org/pdf/2512.05103v1 \"HTTP/1.1 200 OK\"\n",
      "2025-12-06 12:04:15,389 - INFO - HTTP Request: GET https://arxiv.org/pdf/2512.05117v1 \"HTTP/1.1 200 OK\"\n",
      "2025-12-06 12:04:15,390 - INFO - HTTP Request: GET https://arxiv.org/pdf/2512.05112v1 \"HTTP/1.1 200 OK\"\n",
      "2025-12-06 12:04:15,391 - INFO - HTTP Request: GET https://arxiv.org/pdf/2512.05110v1 \"HTTP/1.1 200 OK\"\n",
      "2025-12-06 12:04:16,952 - INFO - Successfully downloaded to 2512.05105v1.pdf\n",
      "2025-12-06 12:04:16,953 - INFO - PDF path for {paper.arxiv_id}: {pdf_path}\n",
      "2025-12-06 12:04:16,956 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-12-06 12:04:16,962 - INFO - Going to convert document batch...\n",
      "2025-12-06 12:04:16,963 - INFO - Processing document 2512.05105v1.pdf\n",
      "2025-12-06 12:04:19,655 - INFO - Finished converting document 2512.05105v1.pdf in 2.70 sec.\n",
      "2025-12-06 12:04:19,666 - WARNING - Parameter `strict_text` has been deprecated and will be ignored.\n",
      "2025-12-06 12:04:19,667 - INFO - Parsed data/arxiv_pdfs/2512.05105v1.pdf\n",
      "2025-12-06 12:04:19,671 - INFO - Downloaded pdf path for paper: 2512.05100v1: data/arxiv_pdfs/2512.05100v1.pdf\n",
      "2025-12-06 12:04:19,672 - INFO - Downloading PDF from https://arxiv.org/pdf/2512.05100v1\n",
      "2025-12-06 12:04:22,745 - INFO - HTTP Request: GET https://arxiv.org/pdf/2512.05100v1 \"HTTP/1.1 200 OK\"\n",
      "2025-12-06 12:04:26,105 - INFO - Successfully downloaded to 2512.05100v1.pdf\n",
      "2025-12-06 12:04:26,105 - INFO - PDF path for {paper.arxiv_id}: {pdf_path}\n",
      "2025-12-06 12:04:26,107 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-12-06 12:04:26,111 - INFO - Going to convert document batch...\n",
      "2025-12-06 12:04:26,111 - INFO - Processing document 2512.05100v1.pdf\n",
      "2025-12-06 12:04:43,607 - INFO - Finished converting document 2512.05100v1.pdf in 17.50 sec.\n",
      "2025-12-06 12:04:43,623 - WARNING - Parameter `strict_text` has been deprecated and will be ignored.\n",
      "2025-12-06 12:04:43,629 - INFO - Parsed data/arxiv_pdfs/2512.05100v1.pdf\n",
      "2025-12-06 12:04:43,630 - INFO - Downloaded pdf path for paper: 2512.05098v1: data/arxiv_pdfs/2512.05098v1.pdf\n",
      "2025-12-06 12:04:43,630 - INFO - Downloading PDF from https://arxiv.org/pdf/2512.05098v1\n",
      "2025-12-06 12:04:46,697 - INFO - HTTP Request: GET https://arxiv.org/pdf/2512.05098v1 \"HTTP/1.1 200 OK\"\n",
      "2025-12-06 12:04:54,296 - INFO - Successfully downloaded to 2512.05103v1.pdf\n",
      "2025-12-06 12:04:54,296 - INFO - PDF path for {paper.arxiv_id}: {pdf_path}\n",
      "2025-12-06 12:04:54,303 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-12-06 12:04:54,323 - INFO - Going to convert document batch...\n",
      "2025-12-06 12:04:54,325 - INFO - Processing document 2512.05103v1.pdf\n",
      "2025-12-06 12:05:01,734 - INFO - Finished converting document 2512.05103v1.pdf in 7.44 sec.\n",
      "2025-12-06 12:05:01,753 - WARNING - Parameter `strict_text` has been deprecated and will be ignored.\n",
      "2025-12-06 12:05:01,756 - INFO - Parsed data/arxiv_pdfs/2512.05103v1.pdf\n",
      "2025-12-06 12:05:01,757 - INFO - Downloaded pdf path for paper: 2512.05073v1: data/arxiv_pdfs/2512.05073v1.pdf\n",
      "2025-12-06 12:05:01,758 - INFO - Downloading PDF from https://arxiv.org/pdf/2512.05073v1\n",
      "2025-12-06 12:05:04,832 - INFO - HTTP Request: GET https://arxiv.org/pdf/2512.05073v1 \"HTTP/1.1 200 OK\"\n",
      "2025-12-06 12:05:07,945 - INFO - Successfully downloaded to 2512.05117v1.pdf\n",
      "2025-12-06 12:05:07,946 - INFO - PDF path for {paper.arxiv_id}: {pdf_path}\n",
      "2025-12-06 12:05:07,956 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-12-06 12:05:07,986 - INFO - Going to convert document batch...\n",
      "2025-12-06 12:05:07,988 - INFO - Processing document 2512.05117v1.pdf\n",
      "2025-12-06 12:06:01,314 - INFO - Finished converting document 2512.05117v1.pdf in 53.37 sec.\n",
      "2025-12-06 12:06:01,344 - WARNING - Parameter `strict_text` has been deprecated and will be ignored.\n",
      "2025-12-06 12:06:01,351 - INFO - Parsed data/arxiv_pdfs/2512.05117v1.pdf\n",
      "2025-12-06 12:06:01,353 - INFO - Downloaded pdf path for paper: 2512.05066v1: data/arxiv_pdfs/2512.05066v1.pdf\n",
      "2025-12-06 12:06:01,353 - INFO - Downloading PDF from https://arxiv.org/pdf/2512.05066v1\n",
      "2025-12-06 12:06:01,428 - INFO - Successfully downloaded to 2512.05073v1.pdf\n",
      "2025-12-06 12:06:01,428 - INFO - PDF path for {paper.arxiv_id}: {pdf_path}\n",
      "2025-12-06 12:06:01,434 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-12-06 12:06:01,442 - INFO - Going to convert document batch...\n",
      "2025-12-06 12:06:01,443 - INFO - Processing document 2512.05073v1.pdf\n",
      "2025-12-06 12:06:09,183 - INFO - Finished converting document 2512.05073v1.pdf in 7.75 sec.\n",
      "2025-12-06 12:06:09,193 - WARNING - Parameter `strict_text` has been deprecated and will be ignored.\n",
      "2025-12-06 12:06:09,195 - INFO - Parsed data/arxiv_pdfs/2512.05073v1.pdf\n",
      "2025-12-06 12:06:09,196 - INFO - Downloaded pdf path for paper: 2512.05058v1: data/arxiv_pdfs/2512.05058v1.pdf\n",
      "2025-12-06 12:06:09,196 - INFO - Downloading PDF from https://arxiv.org/pdf/2512.05058v1\n",
      "2025-12-06 12:06:09,247 - INFO - HTTP Request: GET https://arxiv.org/pdf/2512.05066v1 \"HTTP/1.1 200 OK\"\n",
      "2025-12-06 12:06:09,646 - INFO - Successfully downloaded to 2512.05066v1.pdf\n",
      "2025-12-06 12:06:09,648 - INFO - PDF path for {paper.arxiv_id}: {pdf_path}\n",
      "2025-12-06 12:06:09,652 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-12-06 12:06:09,657 - INFO - Going to convert document batch...\n",
      "2025-12-06 12:06:09,658 - INFO - Processing document 2512.05066v1.pdf\n",
      "2025-12-06 12:06:11,773 - INFO - Finished converting document 2512.05066v1.pdf in 2.12 sec.\n",
      "2025-12-06 12:06:11,783 - WARNING - Parameter `strict_text` has been deprecated and will be ignored.\n",
      "2025-12-06 12:06:11,788 - INFO - Parsed data/arxiv_pdfs/2512.05066v1.pdf\n",
      "2025-12-06 12:06:12,248 - INFO - HTTP Request: GET https://arxiv.org/pdf/2512.05058v1 \"HTTP/1.1 200 OK\"\n",
      "2025-12-06 12:06:15,222 - INFO - Successfully downloaded to 2512.05112v1.pdf\n",
      "2025-12-06 12:06:15,223 - INFO - PDF path for {paper.arxiv_id}: {pdf_path}\n",
      "2025-12-06 12:06:15,224 - WARNING - PDF file size (20.2MB) exceeds limit (20.0MB), skipping peocessing\n",
      "2025-12-06 12:06:15,227 - ERROR - Pipeline error for 2512.05112v1: PDF file size (20.2MB) exceeds limit (20.0MB), skipping peocessing\n",
      "2025-12-06 12:06:15,477 - INFO - Successfully downloaded to 2512.05058v1.pdf\n",
      "2025-12-06 12:06:15,478 - INFO - PDF path for {paper.arxiv_id}: {pdf_path}\n",
      "2025-12-06 12:06:15,484 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-12-06 12:06:15,495 - INFO - Going to convert document batch...\n",
      "2025-12-06 12:06:15,497 - INFO - Processing document 2512.05058v1.pdf\n",
      "2025-12-06 12:06:19,734 - INFO - Finished converting document 2512.05058v1.pdf in 4.25 sec.\n",
      "2025-12-06 12:06:19,744 - WARNING - Parameter `strict_text` has been deprecated and will be ignored.\n",
      "2025-12-06 12:06:19,747 - INFO - Parsed data/arxiv_pdfs/2512.05058v1.pdf\n",
      "2025-12-06 12:06:25,385 - INFO - Successfully downloaded to 2512.05110v1.pdf\n",
      "2025-12-06 12:06:25,385 - INFO - PDF path for {paper.arxiv_id}: {pdf_path}\n",
      "2025-12-06 12:06:25,386 - WARNING - PDF file size (28.9MB) exceeds limit (20.0MB), skipping peocessing\n",
      "2025-12-06 12:06:25,386 - ERROR - Pipeline error for 2512.05110v1: PDF file size (28.9MB) exceeds limit (20.0MB), skipping peocessing\n",
      "2025-12-06 12:06:25,819 - INFO - Successfully downloaded to 2512.05098v1.pdf\n",
      "2025-12-06 12:06:25,820 - INFO - PDF path for {paper.arxiv_id}: {pdf_path}\n",
      "2025-12-06 12:06:25,821 - WARNING - PDF file size (22.0MB) exceeds limit (20.0MB), skipping peocessing\n",
      "2025-12-06 12:06:25,823 - ERROR - Pipeline error for 2512.05098v1: PDF file size (22.0MB) exceeds limit (20.0MB), skipping peocessing\n",
      "2025-12-06 12:06:25,824 - INFO - PDF processing: 1/10 downloaded, 1 papers\n",
      "2025-12-06 12:06:25,826 - ERROR - Pipeline error for 2512.05112v1: Pipeline error for 2512.05112v1, continuing with metadata only\n",
      "2025-12-06 12:06:25,827 - INFO - PDF processing: 1/10 downloaded, 1 papers\n",
      "2025-12-06 12:06:25,827 - ERROR - Pipeline error for 2512.05110v1: Pipeline error for 2512.05110v1, continuing with metadata only\n",
      "2025-12-06 12:06:25,827 - INFO - PDF processing: 1/10 downloaded, 1 papers\n",
      "2025-12-06 12:06:25,828 - INFO - PDF processing: 2/10 downloaded, 2 papers\n",
      "2025-12-06 12:06:25,828 - INFO - PDF processing: 3/10 downloaded, 3 papers\n",
      "2025-12-06 12:06:25,829 - INFO - PDF processing: 4/10 downloaded, 4 papers\n",
      "2025-12-06 12:06:25,829 - ERROR - Pipeline error for 2512.05098v1: Pipeline error for 2512.05098v1, continuing with metadata only\n",
      "2025-12-06 12:06:25,830 - INFO - PDF processing: 4/10 downloaded, 4 papers\n",
      "2025-12-06 12:06:25,830 - INFO - PDF processing: 5/10 downloaded, 5 papers\n",
      "2025-12-06 12:06:25,831 - INFO - PDF processing: 6/10 downloaded, 6 papers\n",
      "2025-12-06 12:06:25,832 - INFO - PDF processing: 7/10 downloaded, 7 papers\n",
      "2025-12-06 12:06:25,833 - INFO - Storing papers to database\n",
      "2025-12-06 12:06:25,868 - INFO - Committed 1 papers to database with full content storage\n",
      "2025-12-06 12:06:25,873 - INFO - Committed 2 papers to database with full content storage\n",
      "2025-12-06 12:06:25,876 - INFO - Committed 3 papers to database with full content storage\n",
      "2025-12-06 12:06:25,883 - INFO - Committed 4 papers to database with full content storage\n",
      "2025-12-06 12:06:25,896 - INFO - Committed 5 papers to database with full content storage\n",
      "2025-12-06 12:06:25,906 - INFO - Committed 6 papers to database with full content storage\n",
      "2025-12-06 12:06:25,909 - INFO - Committed 7 papers to database with full content storage\n",
      "2025-12-06 12:06:25,915 - INFO - Committed 8 papers to database with full content storage\n",
      "2025-12-06 12:06:25,921 - INFO - Committed 9 papers to database with full content storage\n",
      "2025-12-06 12:06:25,927 - INFO - Committed 10 papers to database with full content storage\n",
      "2025-12-06 12:06:25,928 - INFO - Pipeline completed in 133.7s: 10 papers, 7 PDFs, 3 errors\n",
      "2025-12-06 12:06:25,928 - WARNING - Errors summary:\n",
      "2025-12-06 12:06:25,928 - WARNING - 1.Pipeline error for 2512.05112v1: Pipeline error for 2512.05112v1, continuing with metadata only\n",
      "2025-12-06 12:06:25,928 - WARNING - 2.Pipeline error for 2512.05110v1: Pipeline error for 2512.05110v1, continuing with metadata only\n",
      "2025-12-06 12:06:25,928 - WARNING - 3.Pipeline error for 2512.05098v1: Pipeline error for 2512.05098v1, continuing with metadata only\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PIPELINE RESULTS:\n",
      "Papers fetched: 10\n",
      "PDFs downloaded: 7\n",
      "PDFs parsed: 7\n",
      "Papers stored: 10\n",
      "Processing time: 133.7s\n",
      "Errors: 3\n",
      "\n",
      "Errors encountered:\n",
      "   - Pipeline error for 2512.05112v1: Pipeline error for 2512.05112v1, continuing with metadata only\n",
      "   - Pipeline error for 2512.05110v1: Pipeline error for 2512.05110v1, continuing with metadata only\n",
      "   - Pipeline error for 2512.05098v1: Pipeline error for 2512.05098v1, continuing with metadata only\n",
      "\n",
      "Pipeline test successful!\n"
     ]
    }
   ],
   "source": [
    "# Test Complete Pipeline\n",
    "from src.services.metadata_fetcher import make_metadata_fetcher\n",
    "\n",
    "print(\"Test 6: Complete Metadata Fetcher Pipeline\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create metadata fetcher\n",
    "metadata_fetcher = make_metadata_fetcher(arxiv_client, pdf_parser)\n",
    "print(\"✓ Metadata fetcher service created\")\n",
    "\n",
    "# Test with small batch\n",
    "print(\"Running small batch test (2 papers, no PDF processing for speed)...\")\n",
    "\n",
    "try:\n",
    "    with database.get_session() as session:\n",
    "        results = await metadata_fetcher.fetch_and_process_papers(\n",
    "            max_results=10, process_pdfs=True, store_to_db=True, db_session=session\n",
    "        )\n",
    "\n",
    "    print(\"\\nPIPELINE RESULTS:\")\n",
    "    print(f\"Papers fetched: {results.get('papers_fetched', 0)}\")\n",
    "    print(f\"PDFs downloaded: {results.get('pdfs_downloaded', 0)}\")\n",
    "    print(f\"PDFs parsed: {results.get('pdfs_parsed', 0)}\")\n",
    "    print(f\"Papers stored: {results.get('papers_stored', 0)}\")\n",
    "    print(f\"Processing time: {results.get('processing_time', 0):.1f}s\")\n",
    "    print(f\"Errors: {len(results.get('errors', []))}\")\n",
    "\n",
    "    if results.get(\"errors\"):\n",
    "        print(\"\\nErrors encountered:\")\n",
    "        for error in results.get(\"errors\", [])[:3]:  # Show first 3 errors\n",
    "            print(f\"   - {error}\")\n",
    "\n",
    "    if results.get(\"papers_fetched\", 0) > 0:\n",
    "        print(\"\\nPipeline test successful!\")\n",
    "    else:\n",
    "        print(\"\\nNo papers fetched - may be arXiv API unavailability\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Pipeline error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research-assistant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
