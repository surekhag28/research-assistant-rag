{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b84551ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WEEK 2 PREREQUISITE CHECK\n",
      "==================================================\n",
      "✓ FastAPI: Healthy\n",
      "✓ PostgreSQL (via API): Healthy\n",
      "✓ Ollama: Healthy\n",
      "✓ OpenSearch: Healthy\n",
      "✓ Airflow: Healthy\n",
      "\n",
      "All services healthy! Ready for Week 2 development.\n"
     ]
    }
   ],
   "source": [
    "# Test Service Connectivity\n",
    "import requests\n",
    "import subprocess\n",
    "import json\n",
    "\n",
    "services_to_test = {\n",
    "    \"FastAPI\": \"http://localhost:8000/api/v1/health\",\n",
    "    \"PostgreSQL (via API)\": \"http://localhost:8000/api/v1/health\",\n",
    "    \"Ollama\": \"http://localhost:11434/api/version\",\n",
    "    \"OpenSearch\": \"http://localhost:9200/_cluster/health\",\n",
    "    \"Airflow\": \"http://localhost:8080/health\",\n",
    "}\n",
    "\n",
    "print(\"WEEK 2 PREREQUISITE CHECK\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "all_healthy = True\n",
    "\n",
    "for service_name, url in services_to_test.items():\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            print(f\"✓ {service_name}: Healthy\")\n",
    "        else:\n",
    "            print(f\"✗ {service_name}: HTTP {response.status_code}\")\n",
    "            all_healthy = False\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(f\"✗ {service_name}: Not accessible\")\n",
    "        all_healthy = False\n",
    "    except Exception as e:\n",
    "        print(f\"✗ {service_name}: {type(e).__name__}\")\n",
    "        all_healthy = False\n",
    "\n",
    "print()\n",
    "if all_healthy:\n",
    "    print(\"All services healthy! Ready for Week 2 development.\")\n",
    "else:\n",
    "    print(\"Some services need attention. Check Week 1 notebook.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e2c62bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys.path[0:3]: ['/Users/surekha/Documents/projects/RAG/research-assistant/src', '/Users/surekha/Documents/projects/RAG/research-assistant', '/Users/surekha/.local/share/uv/python/cpython-3.12.12-macos-aarch64-none/lib/python312.zip']\n"
     ]
    }
   ],
   "source": [
    "# ensure repo root and src are on sys.path\n",
    "import sys, pathlib\n",
    "\n",
    "repo_root = pathlib.Path().resolve()\n",
    "# if notebooks live in a folder, adjust repo_root = repo_root.parent etc.\n",
    "if (repo_root / \"src\").exists():\n",
    "    sys.path.insert(0, str(repo_root))\n",
    "    sys.path.insert(0, str(repo_root / \"src\"))\n",
    "else:\n",
    "    for p in repo_root.parents:\n",
    "        if (p / \"src\").exists():\n",
    "            sys.path.insert(0, str(p))\n",
    "            sys.path.insert(0, str(p / \"src\"))\n",
    "            break\n",
    "print(\"sys.path[0:3]:\", sys.path[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "beb743b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing ARXIV API Client\n",
      "========================================\n",
      " Client created: https://export.arxiv.org/api/query\n",
      " Rate limit: 3.0\n",
      " Max results: 15\n",
      " Category: cs.AI\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from src.services.arxiv.factory import make_arxiv_client\n",
    "\n",
    "print(\"Testing ARXIV API Client\")\n",
    "print(\"=\" * 40)\n",
    "arxiv_client = make_arxiv_client()\n",
    "print(f\" Client created: {arxiv_client.base_url}\")\n",
    "print(f\" Rate limit: {arxiv_client.rate_limit_delay}\")\n",
    "print(f\" Max results: {arxiv_client.max_results}\")\n",
    "print(f\" Category: {arxiv_client.search_category}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6fcb191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1: Fetch Recent cs.AI papers\n",
      "Fetched 1 papers\n",
      "   1. [2511.16674v1] Dataset Distillation for Pre-Trained Self-Supervised Vision ...\n",
      "      Authors: George Cazenavette, Antonio Torralba...\n",
      "      Categories: cs.CV, cs.AI, cs.LG\n",
      "      Published: 2025-11-20T18:59:57Z\n",
      "\n"
     ]
    }
   ],
   "source": [
    "async def test_fetch_papers():\n",
    "    \"\"\"Testing fetching papers from arxiv client with rate limiting\"\"\"\n",
    "\n",
    "    print(\"Test 1: Fetch Recent cs.AI papers\")\n",
    "    try:\n",
    "        papers = await arxiv_client.fetch_papers(\n",
    "            max_results=1, sort_by=\"submittedDate\", sort_order=\"descending\"\n",
    "        )\n",
    "\n",
    "        print(f\"Fetched {len(papers)} papers\")\n",
    "\n",
    "        if papers:\n",
    "            for i,paper in enumerate(papers[:2],1):\n",
    "                print(f\"   {i}. [{paper.arxiv_id}] {paper.title[:60]}...\")\n",
    "                print(f\"      Authors: {', '.join(paper.authors[:2])}{'...' if len(paper.authors) > 2 else ''}\")\n",
    "                print(f\"      Categories: {', '.join(paper.categories)}\")\n",
    "                print(f\"      Published: {paper.published_date}\")\n",
    "                print()\n",
    "\n",
    "        return papers\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching papers: {e}\")\n",
    "        if \"503\" in str(e):\n",
    "            print(\" arxiv API temporarily unavailable(normal)\")\n",
    "            print(\" Rate limiting and error handling working correctly\")\n",
    "        return []\n",
    "\n",
    "papers = await test_fetch_papers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "890bb0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 2: Date Range Filtering\n",
      " Date filtering test: 2 papers from 20250808-20250809\n",
      "   1. [2508.07111v1] Investigating Intersectional Bias in Large Language Models u...\n",
      "      Authors: Falaah Arif Khan, Nivedha Sivakumar...\n",
      "      Categories: cs.CL, cs.AI\n",
      "      Published: 2025-08-09T22:24:40Z\n",
      "\n",
      "   2. [2508.07107v2] Designing a Feedback-Driven Decision Support System for Dyna...\n",
      "      Authors: Timothy Oluwapelumi Adeyemi, Nadiah Fahad AlOtaibi\n",
      "      Categories: cs.AI, cs.CY\n",
      "      Published: 2025-08-09T21:24:54Z\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test date filtering\n",
    "async def test_date_filtering():\n",
    "\n",
    "    print(\"Test 2: Date Range Filtering\")\n",
    "\n",
    "    from_date = \"20250808\"\n",
    "    to_date = \"20250809\"\n",
    "\n",
    "    try:\n",
    "        date_papers = await arxiv_client.fetch_papers(max_results=2, from_date=from_date,to_date=to_date)\n",
    "\n",
    "        print(f\" Date filtering test: {len(date_papers)} papers from {from_date}-{to_date}\")\n",
    "        if date_papers:\n",
    "            for i, paper in enumerate(date_papers, 1):\n",
    "                print(f\"   {i}. [{paper.arxiv_id}] {paper.title[:60]}...\")\n",
    "                print(\n",
    "                    f\"      Authors: {', '.join(paper.authors[:2])}{'...' if len(paper.authors) > 2 else ''}\"\n",
    "                )\n",
    "                print(f\"      Categories: {', '.join(paper.categories)}\")\n",
    "                print(f\"      Published: {paper.published_date}\")\n",
    "                print()\n",
    "\n",
    "        return date_papers\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Date filtering error: {e}\")\n",
    "        return []\n",
    "\n",
    "# Run date filtering test\n",
    "date_papers = await test_date_filtering()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964dbb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 3: PDF Download & Caching\n",
      "Testing PDF download for : 2508.07111v1\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'title'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     25\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPDF download failed\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     26\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m pdf_path = \u001b[38;5;28;01mawait\u001b[39;00m test_pdf_download(date_papers[:\u001b[32m1\u001b[39m])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mtest_pdf_download\u001b[39m\u001b[34m(test_papers)\u001b[39m\n\u001b[32m      9\u001b[39m test_paper = test_papers[\u001b[32m0\u001b[39m]\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTesting PDF download for : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_paper.arxiv_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTitle: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mtest_papers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtitle\u001b[49m[:\u001b[32m60\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     14\u001b[39m     pdf_path = \u001b[38;5;28;01mawait\u001b[39;00m arxiv_client.download_pdf(test_paper)\n",
      "\u001b[31mAttributeError\u001b[39m: 'list' object has no attribute 'title'"
     ]
    }
   ],
   "source": [
    "# test pdf download and caching locally\n",
    "async def test_pdf_download(test_papers):\n",
    "\n",
    "    print(\"Test 3: PDF Download & Caching\")\n",
    "\n",
    "    if not test_papers:\n",
    "        print(\"No papers available for PDF download test\")\n",
    "\n",
    "    test_paper = test_papers[0]\n",
    "    print(f\"Testing PDF download for : {test_paper.arxiv_id}\")\n",
    "    print(f\"Title: {test_paper.title[:60]}\")\n",
    "\n",
    "    try:\n",
    "        pdf_path = await arxiv_client.download_pdf(test_paper)\n",
    "\n",
    "        if pdf_path and pdf_path.exists():\n",
    "            size_mb = pdf_path.stat().st_size / (1024*1024)\n",
    "            print(f\"PDF downloaded {pdf_path.name} ({size_mb:.2f}) MB\")\n",
    "            return pdf_path\n",
    "        else:\n",
    "            print(\"PDF downlod failed\")\n",
    "            return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"PDF download failed\")\n",
    "        return None\n",
    "    \n",
    "pdf_path = await test_pdf_download(date_papers[:1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research-assistant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
